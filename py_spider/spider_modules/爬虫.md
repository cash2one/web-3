###网络爬虫————一个在网上到处或定向抓取特定网站网页的HTML数据的程序
#####URL————统一资源定位符（网址）
统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。

互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。

######URL的格式由三部分组成
- 第一部分是协议(或称为服务方式)；
- 第二部分是存有该资源的主机IP地址(有时也包括端口号)；
- 第三部分是主机资源的具体地址，如目录和文件名等；

#####一个网站的网页很多，而我们又不可能事先知道所有网页的URL地址
- 定义一个入口页面；
- 从当前页面获取其他页面的URL加入到爬虫的抓取队列中；
- 然后进入到新页面后再递归进行上述操作；

#####Referer
header的一部分，当浏览器向web服务器发送请求的时候，一般会带上Referer，告诉服务器我是从哪个页面链接过来的，服务器藉此可以获得一些信息用于处理。

#####urllib
- localhost()————返回本机IP
- urlencode(dict)————把字典转化为以连接符&划分；
- quote(s, safe='/')————转化url字符编码，safe是不编码的字符；
- quote_plus(s, safe='')————同quote，但是把' '转化为+；
- unquote(s)————取消url编码；
- unquote_plus(s)————同unquote，但是把+转化为' '；
- urlopen(url, data=None, proxies=None)————打开一个URL地址，返回一个类文件对象；
    + data————post请求参数（Content-Type:application/x-www-form-urlencoded）；
    + 类文件对象提供的方法：
        * read()
        * readline()
        * readlines()
        * fileno()
        * close()
        * info()————返回一个httplib.HTTPMessage对象（远程服务器返回的头信息）
        * getcode()————返回Http状态码（200请求成功完成，404网址未找到）
        * geturl()————返回请求的url
- urlretrieve(url, filename=None, reporthook=None, data=None)
    + 将url定位到的html文件下载到本地硬盘中；
    + 如果不指定filename，则会存为当前目录下的临时文件；
    + 返回一个二元组(filename, httplib.HTTPMessage对象)；
- urlcleanup()————清除urllib.urlretrieve()所产生的缓存；

#####urllib2
- request = class Request————构造Requset
    + __init__(
        self,
        url,
        data=None,
        headers={},——————————————构造headers（文件编码、压缩方式、agent等）
                                 也可以用Request.add_header(key, val)追加
                                 还可以在headers中加入referer，对付防盗链
        origin_req_host=None,————
        unverifiable=False
        )
- 发送请求，返回response对象
    + response = urlopen(
        url,————————————————————————URL，必填，http开头的，要写全
        data=None,——————————————————访问URL时要传送的数据，可以使用urllib.urlencode()构造数据
        timeout=<object object>,————设置超时时间（应对一些网站响应过慢），默认为socket._GLOBAL_DEFAULT_TIMEOUT
        cafile=None,
        capath=None,
        cadefault=False,
        context=None
        )
    + 或者response = urlopen(Request)
- response instance————执行urlopen方法之后，返回一个运行时的实例，保存信息
    + response.code————————————————返回状态码
    + read(self, size=-1)——————————获取请求体内容（str，size指定字符串个数，-1表示所有）
    + response.headers.values()————获取请求头信息（list）

#####requests————API比urllib2简单————自动持久连接
- get(url, params=None, **kwargs)————————————————————————return: :class:`Response <Response>` object
    + params————字典参数
    + allow_redirects=False——————————————————禁止重定向
    + timeout=0.001——————————————————————————设置超时时间
    + proxies={'http': ..., 'https': ...}————使用代理
    + headers={...}——————————————————————————定义请求头
- post(url, data=None, json=None, **kwargs)——————————————return: :class:`Response <Response>` object
    + files={'file': open('**/**/***', 'rb')}————上传文件
- put(url, data=None, **kwargs)——————————————————————————return: :class:`Response <Response>` object
- delete(url, **kwargs)——————————————————————————————————return: :class:`Response <Response>` object
- head(url, **kwargs)————————————————————————————————————return: :class:`Response <Response>` object
- options(url, **kwargs)—————————————————————————————————return: :class:`Response <Response>` object
- r————Response object
    + r.text—————————————————————获取网页内容————————unicode
    + r.content——————————————————获取网页内容————————str
    + r.encoding—————————————————获取网页编码————————str
    + r.status_code——————————————获取网页状态码——————str
    + r.history——————————————————获取重定向状态码————list
    + r.json()[key][key][...]————获取json数据————————类字典对象
    + r.headers——————————————————获取响应头——————————类字典对象
    + r.request.headers——————————获取请求头——————————类字典对象
- packages.urllib3.util.parse_url(url)————解析url